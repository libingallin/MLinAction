{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测数值型数据：回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**主要内容**\n",
    "> 线性回归\n",
    "\n",
    "> 局部加权线性回归\n",
    "\n",
    "> 岭回归、Lasso回归和逐步线性回归\n",
    "\n",
    "> 预测鲍鱼年龄和玩具售价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 标准线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**线性回归**</span>\n",
    "+ 优点：结果易于理解，计算算不上复杂\n",
    "+ 缺点：对非线性的数据拟合不好\n",
    "+ 使用数据类型：数值型和标称型数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回归的目的是预测数值型的目标值。\n",
    "\n",
    "$y = a_1*w_1 + a_2*w_2$  \n",
    "这就是所谓的**回归方程(regression equation)**，$a_1$和$a_2$称为**回归系数(regression weights)**，求这些回归系数的过程就是回归。有了这些回归系数，在给定输入，做预测就非常容易了。做法：回归系数乘以输入值，再将结果全部加在一起，就得到了预测值。\n",
    "\n",
    "线性回归(linear regression)意味着可以将输入项分别乘以一些常量，再将结果讲起来得到输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**回归的一般方法**</span>\n",
    "1. 收集数据：采用任意方法收集数据\n",
    "2. 准备数据：回归需要数值型数据，标称型数据将被转化为二值型数据\n",
    "3. 分析数据：绘出数据的可视化二维图有利于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比\n",
    "4. 训练算法：找到回归系数\n",
    "5. 测试算法：使用$R^2$或者预测值和数据的拟合度，来分析模型的效果\n",
    "6. 使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:red\">**标准线性回归(矩阵形式)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一组数据其中包括特征矩阵$X$, 目标变量向量$y$:\n",
    "$$\n",
    "y = {\\left[\n",
    " \\begin{matrix}\n",
    "   y_1\\\\\n",
    "   y_2\\\\\n",
    "   \\vdots\\\\\n",
    "   y_m\n",
    " \\end{matrix}\n",
    " \\right]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X = \n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   1    &x_{11}    &x_{12}  &\\cdots  &x_{1n} \\\\\n",
    "   1    &x_{21}    &x_{22}  &\\cdots  &x_{2n} \\\\\n",
    "   \\vdots &\\vdots   &\\vdots  &\\ddots  &\\vdots \\\\\n",
    "   1    &x_{m1}    &x_{m2}  &\\ddots  &x_{mn}\n",
    " \\end{matrix}\n",
    " \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$X$第一列为截距项，我们做线性回归是为了得到一个最优回归系数向量$w$使得当我们给定一个$x$能够通过$y=xw$预测$y$的值。其中:\n",
    "$$w = \n",
    "{\\left[\n",
    " \\begin{matrix}\n",
    "   w_1\\\\\n",
    "   w_2\\\\\n",
    "   \\vdots\\\\\n",
    "   w_m\n",
    " \\end{matrix}\n",
    " \\right]}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:red\">**最小二乘法获取回归系数**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**找出最优的$w$**。在标准线性回归中需要找到误差最小的$w$，即预测的$y$值与真实的$y$值之间的差值，为了避免简单累加造成的正负差相互抵消，这里采用**平方误差**：\n",
    "$$f(w) = \n",
    "    \\sum_{i=1}^m(y_i-{x_i}^Tw)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上述式子$f(w)$可以通过**梯度下降等方法**得到最优解。使用矩阵表示将会使求解和程序更为简单：\n",
    "$$\n",
    "f(w) = (y-Xw)^T(y-Xw)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(w)$对w求导得：\n",
    "$$\n",
    "\\frac{\\partial f(w)}{\\partial w} = -2X^T(y-Xw)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
