{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 树回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**主要内容**\n",
    "> CART算法\n",
    "\n",
    "> 回归树与模型树\n",
    "\n",
    "> 树剪枝算法\n",
    "\n",
    "> Python中GUI的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归含有一些强大的方法，但这些方法创建的模型需要拟合所有的样本点（局部加权线性回归除外）。当数据拥有众多特征且特征之间关系十分复杂时，构建全局模型的想法就显得困难且笨拙。实际中很多问题都是非线性的，被=不可能使用全局线性模型来拟合任何数据。\n",
    "\n",
    "一种可行的方法是**将数据集切分成很多易于建模的数据，然后利用线性回归技术来建模**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 复杂数据的局部建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分类与回归树(classification and regression tree, CART)**模型同样由特征选择、树的生成和剪枝组成，既可以用于分类也可用于回归。\n",
    "\n",
    "前面利用决策树来进行分类。决策树不断将数据切分成小数据集，直到所有目标变量完全相同或者数据不能再切分为止。决策树是一种**贪心算法**，要在给定时间内做出最佳选择，但并不关心能否达到全局最优。\n",
    "\n",
    "ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有4个取值，那么数据将被切为4份。一旦按某种特征切分后，该特征在之后的算法执行过程中将不会再起作用，所有有观点认为**这种切分方式过于迅速**。另外一种方法是**二元切分发**，即每次把数据集切成两份。如果数据的某个特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之进入右子树。\n",
    "\n",
    "除切分过于迅速外，ID3还有一个问题，它**不能直接处理连续型特征**。只有事先将连续型数值转换成离散型，才能使用ID3算法。但这种**转换过程会破坏连续型变量的内在性质**。而使用二元切分法则易于对树构建过程进行调整以处理连续性特征。**具体做法**：如果特征值大于给定的给定值就走左子树，否则就走右子树。另外，二元切分法也节省了树的构建时间，这点意义不大，因为这些树的构建一般是离线完成，时间并非重点关注的因素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">树回归</span>\n",
    "- **优点**：可以对复杂和非线性的数据建模\n",
    "- **缺点**：结果不易理解\n",
    "- **适用数据类型**：数值型和标称型数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">树回归的一般方法</span>\n",
    "1. 收集数据：采用任意方法\n",
    "2. 准备数据：需要数值型的数据，标称型数据应该映射成二值型数据\n",
    "3. 分析数据：会出数据的二维可视化显示结果，以字典方式生成树\n",
    "4. 训练算法：大部分时间都花在叶节点树模型的构建上\n",
    "5. 测试算法：使用测试数据上的$R^2$值来分析模型结果\n",
    "6. 使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 CART算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是二叉树，内部节点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右子树是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">CART算法由以下两步组成：</span>\n",
    "1. **决策树的生成**：基于训练数据集生成决策树，生成的决策树要尽量大；\n",
    "2. **决策树剪枝**：用验证数据集对生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的生成就是递归地构建二叉决策树的过程。对回归树用**平方误差最小化准则**，对分类树用**基尼指数(Gini index)最小化准则**，进行特征先择，生成二叉树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">回归树的生成</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$X$与$Y$分别作为输入和输出变量，并且$Y$时连续变量，给定训练数据集$$D = \\left\\{(x_1,y_1), (x_2,y_2), \\cdots, (x_N, y_N)\\right\\}$$\n",
    "\n",
    "考虑如何生成回归树。\n",
    "\n",
    "一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为$M$个单元$R_1, R_, \\cdots, R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树的模型可表示为\n",
    "$$f(x) = \\sum_{m=1}^Mc_mI(x\\in R_m)$$\n",
    "\n",
    "当输入空间的划分确定时，可以用**平方误差**$\\sum_{x_i\\in R_m}(y_i-f(x_i))^2$来表示回归树对于训练数据集的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元$R_m$上的$\\hat{c_m}$的最优值$m$是$R_m$上的所有输出实例$x_i$对应的输出$y_i$的均值，即\n",
    "$$\\hat{c_m} = ave(y_i|x_i \\in R_m)$$\n",
    "\n",
    "问题是怎么**对输入空间进行划分**。这里**采用启发式的方法**，选择第$j$个变量$x^{(j)}$和它的取值$s$，作为切分变量(splitting variable)和切分点(splitting point)，并定义两个区域：\n",
    "$$R_1(j,s) = \\left\\{x|x^{(j)}\\leq s\\right\\},R_2(j,s) = \\left\\{x|x^{(j)}>s\\right\\}$$\n",
    "\n",
    "然后寻找最优切分变量$j$和最优切分点$s$.具体地，求解：\n",
    "\n",
    "$$min_{j,s}\\left[min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2\\right]$$\n",
    "\n",
    "对固定输入变量$j$可以找到最优切分点$s$.\n",
    "$$\\hat{c_1} = ave\\left(y_i|x_i\\in R_1(j,s)\\right),\\hat{c_2} = ave(y_i|x_i\\in R_2(j,s))$$\n",
    "\n",
    "遍历所有输入变量，找到最优地切分变量$j$，构成一个对$(j,s)$.依此将输入空间划分成两个区域。接着，对每个区域重复上述过程，直到满足条件为止。这样生成一棵回归树。这样地回归树通常称为**最小二乘回归树(least squares regression tree)**，算法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">最小二乘树算法</span>\n",
    "\n",
    "+ **输入**：训练数据集D\n",
    "+ **输出**：回归树$f(x)$\n",
    "\n",
    "在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：\n",
    "1. 选择最优切分变量$j$和切分点$s$，求解\n",
    "$$min_{j,s}\\left[min_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2\\right]$$\n",
    "遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的对$(j, s)$.\n",
    "2. 用选定的对$(j, s)$划分区域并决定相应的输出值：\n",
    "  $$R_1(j,s) = \\left\\{x|x^{(j)}\\leq s\\right\\}，R_2(j,s) = \\left\\{x|x^{(j)}>s\\right\\}$$\n",
    "  \n",
    "  $$\\hat{c_m} = \\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i, x\\in R_m, m=1,2$$\n",
    "3. 继续对两个子区域调用步骤1，2，直到停止条件\n",
    "4. 将输入空间划分为$M$个区域$R_1, R_2, \\cdots, R_M$，生成决策树：\n",
    "$$f(x) = \\sum_{m=1}^M \\hat{c_m}I(x\\in R_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 连续和离散型特征的树的构建（CART算法用于回归）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "这里使用一部字典来存储树的数据结构，该字典包含以下4个元素：\n",
    "> 待切分的特征\n",
    "\n",
    "> 带切分的特征值\n",
    "\n",
    "> 右子树。当不需要切分的时候，也可以是单值\n",
    "\n",
    "> 左子树。与右子树类似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    CART算法只做二元切分，所以这里可以固定树的数据结构。树包含左键和右键，可以存储另一棵子树或单个值。字典还包含特征和特征值这两个键，他们给出切分算法所有的特征和特征值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">函数`createTree()`的伪代码</span>\n",
    "```\n",
    "找到最佳的带切分特征：\n",
    "    如果该节点不能再分，将该节点存为叶节点\n",
    "    执行二元切分\n",
    "    在右子树调用createTree()方法\n",
    "    在左子树调用createTree()方法\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet(fileName):\n",
    "    '''General function to parse tab-delimited floats.\n",
    "    Assume laset colum is target value.\n",
    "    '''\n",
    "    dataMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        curLine = line.rstrip().split('\\t')\n",
    "        fltLine = map(float, curLine)  # map all elements to float()\n",
    "        dataMat.append(fltLine)\n",
    "    return np.mat(dataMat)  # np.matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 该函数读取一个以tab键为分隔符的文件，然后将每行的内容保存成一组浮点数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binSplitDataSet(dataSet, feature, value):\n",
    "    '''Split dataSet in two subclass according to given feature and value.\n",
    "    Args:\n",
    "        dataSet: data set, arrar_like\n",
    "        feature: split\n",
    "        value: the value of feature to do the split\n",
    "    Returns:\n",
    "        two subclasses\n",
    "    '''\n",
    "    # more than value\n",
    "    mat0 = dataSet[np.nonzero(dataSet[:, feature] > value)[0], :]\n",
    "    # less than or equal to value\n",
    "    mat1 = dataSet[np.nonzero(dataSet[:, feature] <= value)[0], :]\n",
    "    return mat0, mat1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在给定特征和特征值的情况下，该函数通过数组过滤方式将上述数据集合切分得到两个子集并返回。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归树假设叶节点是常数值，这种策略认为数据中的复杂关系可以用树结构来概括。为成功构建以分段函数为叶节点的树，需要度量出数据的一致性。如何**计算连续性数值的混乱度**呢？首先计算所有数据的均值，然后计算每条数据的值到均值的差值。为了正负差值同等看待，一般使用绝对值或平方值来代替上述差值。这里需要的是平方误差的总值（总方差）。总方差可以通过均方差乘以数据集中样本点的个数来得到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regLeaf(dataSet):\n",
    "    '''Generate leaf node.Returns the value used for each leaf.'''\n",
    "    return np.mean(dataSet[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 负责生成叶节点。当chooseBestSplit()函数确定不再对数据进行切分时，将调用该函数来得到叶节点的模型。在回归树中，该模型其实就是目标变量的均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regErr(dataSet):\n",
    "    '''Calculate the square error of target variable on the given dataset.'''\n",
    "    return np.var(dataSet[:, -1]) * np.shape(dataSet)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 该函数在给定数据集上计算目标变量的平方误差。需要返回的是总方差，所以要用均方差乘以数据集中样本的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">函数`chooseBestSplit()`的伪代码</span>\n",
    "```\n",
    "对每个特征\n",
    "    对每个特征值\n",
    "        将数据集切分成两份\n",
    "        计算切分的误差\n",
    "        如果当前误差小于当前最小误差，那么将当前切分定为最佳切分并更新最小误差\n",
    "返回最佳切分的特征和阈值\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n",
    "    '''Find the best feature and value to split dataSet.'''\n",
    "    # parameters pointed by user\n",
    "    tolS = ops[0]  # tolerant error decrease\n",
    "    tolN = ops[1]  # minimun sample size\n",
    "    # if all the target variables are the same value: quit and return value\n",
    "    if len(set(dataSet[:, -1].T.tolist()[0])) == 1:  # exit condition 1\n",
    "        return None, leafType(dataSet)\n",
    "    m, n = np.shape(dataSet)\n",
    "    # the choice of the best feature is driven by\n",
    "    # Reduction in RSS(residual sum of squares) error from mean\n",
    "    S = errType(dataSet)\n",
    "    bestS = np.inf  # error\n",
    "    bestIndex = 0  # the best feature to split\n",
    "    bestValue = 0  # the corresponding value\n",
    "    for featIndex in range(n-1):  # traverse every feature\n",
    "        for splitVal in set(dataSet.A[:, featIndex]):  # every value of feature\n",
    "            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)  # split\n",
    "            # if the size of some subclass is less than tolN don't do the split\n",
    "            if (mat0.shape[0] < tolN) or (mat1.shape[0] < tolN):\n",
    "                continue  # jump out of this loop\n",
    "            newS = errType(mat0) + errType(mat1)\n",
    "            if newS < bestS:\n",
    "                bestIndex = featIndex\n",
    "                bestValue = splitVal\n",
    "                bestS = newS\n",
    "    # if the decrease (S-bestS) is less than a threshold(tolS)\n",
    "    # don't do the split\n",
    "    if (S - bestS) < tolS:\n",
    "        return None, leafType(dataSet)  # exit condition 2\n",
    "    # split dataSet according to bestIndex and bestValue\n",
    "    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\n",
    "    # if the size of some subclass is less than tolN, don't do the split\n",
    "    # if (mat0.shape[0] < tolN) or (mat1.shape[0] < tolN):  # I think\n",
    "    # return None, leafType  # exit condition 3  # this for-loop is redundant\n",
    "    # returns the best feature to split on and the value used for that split\n",
    "    return bestIndex, bestValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 构建回归树的核心函数。该函数目的是找到数据的最佳二元切分方式（即切分后能达到最低误差的切分）。如果找不到一个‘好’的二元切分，该函数返回None并同时调用createTree()函数产生叶节点，叶节点的值也将返回None。在函数中有3种情况不会切分（第3种被我注释掉了，因为在for循环中执行了，没必要在循环外再执行一次———我的想法），将直接创建叶节点。如果找到‘好’的切分方式，则返回特征编号和切分特征值。\n",
    "\n",
    "> 函数会统计不同剩余特征值的数目，如果该数目为1， 那么就不需要再切分而直接返回。\n",
    "\n",
    "> ops设定的tolN和tolN，是用户指定的参数，用于控制函数的停止时机。tolS:是容许的误差下降值，即切分数据集后效果提升的不够大，那么就不该进行切分操作而直接创建叶节点。tolN:是切分的最少样本数，即切分后的某个数据子集大小小于用户该值，那么也不应该切分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n",
    "    '''Assume the dataSet is Numpy so we can array filtering.'''\n",
    "    # choose the best split\n",
    "    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)\n",
    "    # if the splitting hit a stop condition return val\n",
    "    if feat is None:\n",
    "        return val\n",
    "    retTree = {}\n",
    "    retTree['spInd'] = feat\n",
    "    retTree['spVal'] = val\n",
    "    lSet, rSet = binSplitDataSet(dataSet, feat, val)  # split\n",
    "    retTree['left'] = createTree(lSet, leafType, errType, ops)\n",
    "    retTree['right'] = createTree(rSet, leafType, errType, ops)\n",
    "    return retTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 是一个递归函数。满足停止条件，chooseBestSplit()将返回None和某类模型的值。如果构建的是回归树，该模型是一个常数；如果是模型树，其模型是一个线性方程。不满足递归条件，在切分的数据子集上递归调用createTree()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left': 1.0180967672413792,\n",
       " 'right': -0.044650285714285719,\n",
       " 'spInd': 0,\n",
       " 'spVal': 0.48813000000000001}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata1 = loadDataSet('ex00.txt')\n",
    "createTree(mydata1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left': {'left': {'left': 3.9871631999999999,\n",
       "   'right': 2.9836209534883724,\n",
       "   'spInd': 1,\n",
       "   'spVal': 0.79758300000000004},\n",
       "  'right': 1.980035071428571,\n",
       "  'spInd': 1,\n",
       "  'spVal': 0.58200200000000002},\n",
       " 'right': {'left': 1.0289583666666666,\n",
       "  'right': -0.023838155555555553,\n",
       "  'spInd': 1,\n",
       "  'spVal': 0.19783400000000001},\n",
       " 'spInd': 1,\n",
       " 'spVal': 0.39434999999999998}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata2 = loadDataSet('ex0.txt')\n",
    "createTree(mydata2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归树的构建已完成，但是需要某种措施来检查构建过程中是否得当。通过**树剪枝(tree pruning)**技术，来通过对决策树剪枝来达到更好的预测效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 树剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据集的分类很准确，对未知数据的测试数据集却没有那么准确，即出现**过拟合**现象。**过拟合的原因**在于学习过多地考虑如何提高对训练数据的正确性，从而构建了复杂的树。解决办法是考虑决策树的复杂度，对以生成的决策树进行简化。\n",
    "\n",
    "通过降低决策树的复杂度来避免过拟合的过程称为**剪枝(pruning)**。具体地讲，剪枝从以生成的树上裁掉一些子树或叶节点，并将其根节点或父节点做为新的叶节点，从而简化分类树模型。\n",
    "\n",
    "在函数`chooseBestSplit()`中提前终止条件，实际上是进行一种所谓的**预剪枝(prepruning)**操作。另一种形式的剪枝需要使用测试集和训练集，称作为**后剪枝(postpruning)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预剪枝**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以不断修改tolS、tolN来得到合理结果，但这并不是好的办法。事实上，我们常常甚至不确定到底需要寻找什么样的结果。\n",
    "\n",
    "由于后剪枝不需要用户指定参数，后剪枝是一个更理想化的剪枝方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**后剪枝**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用后剪枝方法需要将数据集分成测试集和训练集。首先指定参数，使得构建出的树足够大、足够复杂，便于剪枝。接下来从上而下找到叶节点，用测试集来判断将这些叶节点合并是否能降低测试误差。如果是的话就合并。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">函数`pruning()`的伪代码</span>\n",
    "```\n",
    "基于已有的树切分测试数据：\n",
    "    如果存在任一子集是一棵树，则在该子集递归剪枝过程\n",
    "    计算将当前两个叶节点合并后的误差\n",
    "    计算不合并的误差\n",
    "    如果合并会降低误差的话，就将叶节点合并\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isTree(obj):\n",
    "    '''Test if obj is a tree.\n",
    "    Returns: boolean\n",
    "    '''\n",
    "    return (type(obj).__name__ == 'dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 用于测试输入变量是否是一棵树，返回布尔型。即用于判断当前处理的节点是否是叶节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMean(tree):\n",
    "    '''Traverse tree until leafs are finded. If finded, calculate the mean.'''\n",
    "    if isTree(tree['left']):\n",
    "        tree['left'] = getMean(tree['left'])\n",
    "    if isTree(tree['right']):\n",
    "        tree['right'] = getMean(tree['right'])\n",
    "    return (tree['left']+tree['right']) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 递归函数。从上往下遍历树直到叶节点为止。如果找到两个叶节点则计算它们的平均值。该函数对树进行塌陷处理（即返回树平均值），在pruning()函数中调用该函数时应明确这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pruning(tree, testData):\n",
    "    '''Prune tree.\n",
    "    Args:\n",
    "        tree: tree to be pruned\n",
    "        testData: test data set to prune\n",
    "    Returns:\n",
    "        a tree after pruned or mean of tree\n",
    "    '''\n",
    "    if testData.shape[0] == 0:  # if we have no test data collapse the tree\n",
    "        return getMean(tree)\n",
    "    # if the branches are not trees try to prune them\n",
    "    if (isTree(tree['left']) or isTree(tree['right'])):  # have a sub-tree\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n",
    "    if isTree(tree['left']):  # if have a left sub-tree prune it\n",
    "        tree['left'] = pruning(tree['left'], lSet)\n",
    "    if isTree(tree['right']):  # if have a right tree prune it\n",
    "        tree['right'] = pruning(tree['right'], rSet)\n",
    "    # if they are both leafs, see if we can merge them\n",
    "    if not isTree(tree['left']) and not isTree(tree['right']):\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n",
    "        errorNoMerge = np.sum(np.power(lSet[:, -1]-tree['left'], 2)) + \\\n",
    "            np.sum(np.power(rSet[:, -1]-tree['right'], 2))\n",
    "        treeMean = (tree['left']+tree['right']) / 2.0\n",
    "        errorMerge = np.sum(np.power(testData[:, -1]-treeMean, 2))\n",
    "        if errorMerge < errorNoMerge:  # if error decrease merge them\n",
    "            print('Merging!')\n",
    "            return treeMean\n",
    "        else:\n",
    "            return tree\n",
    "    else:\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 参数：待剪枝的树与剪枝所需的测试数据。\n",
    "\n",
    "> 函数首先要确认测试集是否为空。一旦非空，则反复递归调用函数pruning()对测试数据进行切分。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydata3 = loadDataSet('ex2.txt')\n",
    "mytree = createTree(mydata3, ops=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n",
      "Merging!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'left': {'left': {'left': {'left': 92.523991499999994,\n",
       "    'right': {'left': {'left': {'left': 112.386764,\n",
       "       'right': 123.559747,\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.96039799999999997},\n",
       "      'right': 135.83701300000001,\n",
       "      'spInd': 0,\n",
       "      'spVal': 0.95851200000000003},\n",
       "     'right': 111.2013225,\n",
       "     'spInd': 0,\n",
       "     'spVal': 0.956951},\n",
       "    'spInd': 0,\n",
       "    'spVal': 0.96596899999999997},\n",
       "   'right': {'left': {'left': {'left': {'left': {'left': {'left': {'left': {'left': {'left': {'left': {'left': 96.41885225,\n",
       "              'right': 69.318648999999994,\n",
       "              'spInd': 0,\n",
       "              'spVal': 0.94882200000000005},\n",
       "             'right': {'left': {'left': 110.03503850000001,\n",
       "               'right': {'left': 65.548417999999998,\n",
       "                'right': {'left': 115.75399400000001,\n",
       "                 'right': {'left': {'left': 94.396114499999996,\n",
       "                   'right': 85.005351000000005,\n",
       "                   'spInd': 0,\n",
       "                   'spVal': 0.912161},\n",
       "                  'right': {'left': {'left': 106.814667,\n",
       "                    'right': 118.513475,\n",
       "                    'spInd': 0,\n",
       "                    'spVal': 0.90862900000000002},\n",
       "                   'right': {'left': 87.300624999999997,\n",
       "                    'right': {'left': {'left': 100.133819,\n",
       "                      'right': 108.09493399999999,\n",
       "                      'spInd': 0,\n",
       "                      'spVal': 0.90069900000000003},\n",
       "                     'right': {'left': 82.436685999999995,\n",
       "                      'right': {'left': 98.544549499999988,\n",
       "                       'right': 106.16859550000001,\n",
       "                       'spInd': 0,\n",
       "                       'spVal': 0.87219899999999995},\n",
       "                      'spInd': 0,\n",
       "                      'spVal': 0.88842600000000005},\n",
       "                     'spInd': 0,\n",
       "                     'spVal': 0.89299899999999999},\n",
       "                    'spInd': 0,\n",
       "                    'spVal': 0.90142100000000003},\n",
       "                   'spInd': 0,\n",
       "                   'spVal': 0.90144400000000002},\n",
       "                  'spInd': 0,\n",
       "                  'spVal': 0.91097499999999998},\n",
       "                 'spInd': 0,\n",
       "                 'spVal': 0.92578199999999999},\n",
       "                'spInd': 0,\n",
       "                'spVal': 0.93485300000000005},\n",
       "               'spInd': 0,\n",
       "               'spVal': 0.93652400000000002},\n",
       "              'right': {'left': {'left': 89.20993,\n",
       "                'right': 76.240983999999997,\n",
       "                'spInd': 0,\n",
       "                'spVal': 0.84721900000000006},\n",
       "               'right': 95.893130999999997,\n",
       "               'spInd': 0,\n",
       "               'spVal': 0.84294000000000002},\n",
       "              'spInd': 0,\n",
       "              'spVal': 0.85497000000000001},\n",
       "             'spInd': 0,\n",
       "             'spVal': 0.94422099999999998},\n",
       "            'right': 60.552307999999996,\n",
       "            'spInd': 0,\n",
       "            'spVal': 0.84162499999999996},\n",
       "           'right': 124.87935300000001,\n",
       "           'spInd': 0,\n",
       "           'spVal': 0.84154700000000005},\n",
       "          'right': {'left': 76.723834999999994,\n",
       "           'right': {'left': 59.342323,\n",
       "            'right': 70.054507999999998,\n",
       "            'spInd': 0,\n",
       "            'spVal': 0.81972199999999995},\n",
       "           'spInd': 0,\n",
       "           'spVal': 0.82384800000000002},\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.83302600000000004},\n",
       "         'right': {'left': 118.319942,\n",
       "          'right': {'left': 99.841379000000003,\n",
       "           'right': 112.981216,\n",
       "           'spInd': 0,\n",
       "           'spVal': 0.81136299999999995},\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.81160200000000005},\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.81521500000000002},\n",
       "        'right': 73.494399250000001,\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.80615800000000004},\n",
       "       'right': {'left': 114.4008695,\n",
       "        'right': 102.26514075,\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.78686500000000004},\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.79031200000000001},\n",
       "      'right': 64.041940999999994,\n",
       "      'spInd': 0,\n",
       "      'spVal': 0.76904300000000003},\n",
       "     'right': 115.199195,\n",
       "     'spInd': 0,\n",
       "     'spVal': 0.76332800000000001},\n",
       "    'right': 78.085643250000004,\n",
       "    'spInd': 0,\n",
       "    'spVal': 0.75950399999999996},\n",
       "   'spInd': 0,\n",
       "   'spVal': 0.95283300000000004},\n",
       "  'right': {'left': {'left': {'left': {'left': {'left': {'left': {'left': 110.90282999999999,\n",
       "         'right': {'left': 103.345308,\n",
       "          'right': 108.55391899999999,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.71023400000000003},\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.71621100000000004},\n",
       "        'right': 135.41676699999999,\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.70889000000000002},\n",
       "       'right': {'left': {'left': {'left': {'left': 106.18042699999999,\n",
       "           'right': 105.062147,\n",
       "           'spInd': 0,\n",
       "           'spVal': 0.70638999999999996},\n",
       "          'right': 115.58660500000001,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.69987299999999997},\n",
       "         'right': 92.470635999999999,\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.69891999999999999},\n",
       "        'right': {'left': 120.521925,\n",
       "         'right': {'left': 101.91115275,\n",
       "          'right': 112.78136649999999,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.66645200000000004},\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.68909900000000002},\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.69847199999999998},\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.70696099999999995},\n",
       "      'right': {'left': 121.98060700000001,\n",
       "       'right': {'left': 115.687524,\n",
       "        'right': 112.715799,\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.65246199999999999},\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.66107300000000002},\n",
       "      'spInd': 0,\n",
       "      'spVal': 0.66532899999999995},\n",
       "     'right': 82.500765999999999,\n",
       "     'spInd': 0,\n",
       "     'spVal': 0.64270700000000003},\n",
       "    'right': 140.61394100000001,\n",
       "    'spInd': 0,\n",
       "    'spVal': 0.64237299999999997},\n",
       "   'right': {'left': {'left': {'left': {'left': 82.713621000000003,\n",
       "       'right': {'left': 91.656616999999997,\n",
       "        'right': 93.645292999999995,\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.632691},\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.63799899999999998},\n",
       "      'right': {'left': 117.62834599999999,\n",
       "       'right': 105.970743,\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.62482700000000002},\n",
       "      'spInd': 0,\n",
       "      'spVal': 0.62806099999999998},\n",
       "     'right': 82.04976400000001,\n",
       "     'spInd': 0,\n",
       "     'spVal': 0.62390900000000005},\n",
       "    'right': {'left': 168.180746,\n",
       "     'right': {'left': {'left': {'left': {'left': {'left': {'left': 93.521395999999996,\n",
       "           'right': {'left': 130.37852899999999,\n",
       "            'right': {'left': 111.9849935,\n",
       "             'right': {'left': 82.589327999999995,\n",
       "              'right': {'left': 114.872056,\n",
       "               'right': 108.43539199999999,\n",
       "               'spInd': 0,\n",
       "               'spVal': 0.56932700000000003},\n",
       "              'spInd': 0,\n",
       "              'spVal': 0.571214},\n",
       "             'spInd': 0,\n",
       "             'spVal': 0.58231100000000002},\n",
       "            'spInd': 0,\n",
       "            'spVal': 0.58980600000000005},\n",
       "           'spInd': 0,\n",
       "           'spVal': 0.59914199999999995},\n",
       "          'right': 82.903944999999993,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.56030100000000005},\n",
       "         'right': 129.06244849999999,\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.55379699999999998},\n",
       "        'right': {'left': 83.114502000000002,\n",
       "         'right': {'left': 97.340526499999996,\n",
       "          'right': 90.995536000000001,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.53783400000000003},\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.546601},\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.548539},\n",
       "       'right': {'left': {'left': 129.76674299999999,\n",
       "         'right': 124.795495,\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.53194399999999997},\n",
       "        'right': 116.17616200000001,\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.51915},\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.53351099999999996},\n",
       "      'right': {'left': 101.075609,\n",
       "       'right': {'left': 93.292828999999998,\n",
       "        'right': 96.403373000000002,\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.50854200000000005},\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.508548},\n",
       "      'spInd': 0,\n",
       "      'spVal': 0.51333200000000001},\n",
       "     'spInd': 0,\n",
       "     'spVal': 0.60641699999999998},\n",
       "    'spInd': 0,\n",
       "    'spVal': 0.61300399999999999},\n",
       "   'spInd': 0,\n",
       "   'spVal': 0.64051499999999995},\n",
       "  'spInd': 0,\n",
       "  'spVal': 0.72939699999999996},\n",
       " 'right': {'left': {'left': {'left': {'left': {'left': 8.5367700000000006,\n",
       "      'right': 27.729263,\n",
       "      'spInd': 0,\n",
       "      'spVal': 0.48738100000000001},\n",
       "     'right': 5.224234,\n",
       "     'spInd': 0,\n",
       "     'spVal': 0.48380299999999998},\n",
       "    'right': {'left': -9.7129250000000003,\n",
       "     'right': -23.777531,\n",
       "     'spInd': 0,\n",
       "     'spVal': 0.46567999999999998},\n",
       "    'spInd': 0,\n",
       "    'spVal': 0.46738299999999999},\n",
       "   'right': {'left': 30.051931,\n",
       "    'right': 17.171057000000001,\n",
       "    'spInd': 0,\n",
       "    'spVal': 0.46324100000000001},\n",
       "   'spInd': 0,\n",
       "   'spVal': 0.465561},\n",
       "  'right': {'left': -34.044555000000003,\n",
       "   'right': {'left': {'left': {'left': {'left': {'left': -4.1911744999999998,\n",
       "        'right': {'left': {'left': {'left': {'left': 19.745224,\n",
       "            'right': 15.224266,\n",
       "            'spInd': 0,\n",
       "            'spVal': 0.42858200000000002},\n",
       "           'right': -21.594268,\n",
       "           'spInd': 0,\n",
       "           'spVal': 0.42671100000000001},\n",
       "          'right': 44.161493,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.41894300000000001},\n",
       "         'right': {'left': -26.419288999999999,\n",
       "          'right': 0.63593000000000011,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.40322799999999998},\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.41251599999999999},\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.43765199999999999},\n",
       "       'right': 23.197474,\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.388789},\n",
       "      'right': {'left': {'left': {'left': -29.007783,\n",
       "         'right': {'left': {'left': 13.583555,\n",
       "           'right': 5.2411960000000004,\n",
       "           'spInd': 0,\n",
       "           'spVal': 0.37738300000000002},\n",
       "          'right': -8.2282969999999995,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.37350100000000003},\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.378965},\n",
       "        'right': {'left': -32.124495000000003,\n",
       "         'right': {'left': -9.9938275000000001,\n",
       "          'right': -26.851234812500003,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.35072500000000001},\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.35679},\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.37004199999999998},\n",
       "       'right': {'left': 22.286959625000001,\n",
       "        'right': {'left': {'left': -20.397333499999998,\n",
       "          'right': -49.939515999999998,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.31095600000000001},\n",
       "         'right': {'left': {'left': {'left': {'left': {'left': {'left': {'left': {'left': {'left': {'left': 8.8147249999999993,\n",
       "                   'right': {'left': -18.051317999999998,\n",
       "                    'right': {'left': -1.7983769999999999,\n",
       "                     'right': {'left': -14.988279,\n",
       "                      'right': -14.391613,\n",
       "                      'spInd': 0,\n",
       "                      'spVal': 0.29074899999999998},\n",
       "                     'spInd': 0,\n",
       "                     'spVal': 0.29599300000000001},\n",
       "                    'spInd': 0,\n",
       "                    'spVal': 0.29710700000000001},\n",
       "                   'spInd': 0,\n",
       "                   'spVal': 0.30031799999999997},\n",
       "                  'right': {'left': 35.623745999999997,\n",
       "                   'right': {'left': -9.4575560000000003,\n",
       "                    'right': {'left': 5.2805790000000004,\n",
       "                     'right': 2.5579230000000002,\n",
       "                     'spInd': 0,\n",
       "                     'spVal': 0.26463900000000001},\n",
       "                    'spInd': 0,\n",
       "                    'spVal': 0.26492599999999999},\n",
       "                   'spInd': 0,\n",
       "                   'spVal': 0.27386300000000002},\n",
       "                  'spInd': 0,\n",
       "                  'spVal': 0.28479399999999999},\n",
       "                 'right': {'left': {'left': -9.601409499999999,\n",
       "                   'right': -30.812912000000001,\n",
       "                   'spInd': 0,\n",
       "                   'spVal': 0.22875100000000001},\n",
       "                  'right': -2.266273,\n",
       "                  'spInd': 0,\n",
       "                  'spVal': 0.228628},\n",
       "                 'spInd': 0,\n",
       "                 'spVal': 0.25807000000000002},\n",
       "                'right': 6.0992389999999999,\n",
       "                'spInd': 0,\n",
       "                'spVal': 0.22847300000000001},\n",
       "               'right': {'left': -16.427370249999999,\n",
       "                'right': -2.6781804999999999,\n",
       "                'spInd': 0,\n",
       "                'spVal': 0.20216100000000001},\n",
       "               'spInd': 0,\n",
       "               'spVal': 0.21163299999999999},\n",
       "              'right': 9.5773855000000001,\n",
       "              'spInd': 0,\n",
       "              'spVal': 0.19328200000000001},\n",
       "             'right': {'left': {'left': {'left': -14.740059,\n",
       "                'right': -6.5125060000000001,\n",
       "                'spInd': 0,\n",
       "                'spVal': 0.166431},\n",
       "               'right': -27.405211000000001,\n",
       "               'spInd': 0,\n",
       "               'spVal': 0.164134},\n",
       "              'right': 0.225886,\n",
       "              'spInd': 0,\n",
       "              'spVal': 0.156273},\n",
       "             'spInd': 0,\n",
       "             'spVal': 0.166765},\n",
       "            'right': {'left': 7.5573490000000003,\n",
       "             'right': 7.3367839999999998,\n",
       "             'spInd': 0,\n",
       "             'spVal': 0.13988},\n",
       "            'spInd': 0,\n",
       "            'spVal': 0.15606700000000001},\n",
       "           'right': -29.087463,\n",
       "           'spInd': 0,\n",
       "           'spVal': 0.13861899999999999},\n",
       "          'right': 22.478290999999999,\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.13183300000000001},\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.30913299999999999},\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.32427400000000001},\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.33518199999999998},\n",
       "      'spInd': 0,\n",
       "      'spVal': 0.38203700000000002},\n",
       "     'right': -39.524461000000002,\n",
       "     'spInd': 0,\n",
       "     'spVal': 0.13062599999999999},\n",
       "    'right': {'left': 22.891674999999999,\n",
       "     'right': {'left': {'left': 6.1965159999999999,\n",
       "       'right': {'left': -16.106164,\n",
       "        'right': {'left': -1.2931950000000001,\n",
       "         'right': -10.137104000000001,\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.085873000000000005},\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.10796},\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.10880099999999999},\n",
       "      'right': {'left': 37.820658999999999,\n",
       "       'right': {'left': -24.132225999999999,\n",
       "        'right': {'left': 15.824970500000001,\n",
       "         'right': {'left': -15.160836,\n",
       "          'right': {'left': {'left': {'left': 6.6955669999999996,\n",
       "             'right': -3.131497,\n",
       "             'spInd': 0,\n",
       "             'spVal': 0.055862000000000002},\n",
       "            'right': -13.731698,\n",
       "            'spInd': 0,\n",
       "            'spVal': 0.053763999999999999},\n",
       "           'right': 4.0916259999999998,\n",
       "           'spInd': 0,\n",
       "           'spVal': 0.044736999999999999},\n",
       "          'spInd': 0,\n",
       "          'spVal': 0.061219000000000003},\n",
       "         'spInd': 0,\n",
       "         'spVal': 0.068373000000000003},\n",
       "        'spInd': 0,\n",
       "        'spVal': 0.080060999999999993},\n",
       "       'spInd': 0,\n",
       "       'spVal': 0.084661},\n",
       "      'spInd': 0,\n",
       "      'spVal': 0.085111000000000006},\n",
       "     'spInd': 0,\n",
       "     'spVal': 0.124723},\n",
       "    'spInd': 0,\n",
       "    'spVal': 0.126833},\n",
       "   'spInd': 0,\n",
       "   'spVal': 0.45576100000000003},\n",
       "  'spInd': 0,\n",
       "  'spVal': 0.457563},\n",
       " 'spInd': 0,\n",
       " 'spVal': 0.49917099999999998}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytestdata = loadDataSet('ex2test.txt')\n",
    "pruning(mytree, mytestdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般地，为了寻求最佳模型可以同时使用两种剪枝技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 模型树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用树来对数据建模，除了把叶节点简单地为常数值为，还有一种方法是把**叶节点**设定为**分段线性函数**。这里所谓的**分段线性(piecewise linear)**)是指模型由多个线性片组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def pltData():\n",
    "    data = []\n",
    "    fr = open('exp2.txt')\n",
    "    for line in fr.readlines():\n",
    "        curLine = line.rstrip().split('\\t')\n",
    "        fltLine = map(float, curLine)\n",
    "        data.append(fltLine)\n",
    "    data = np.array(data)\n",
    "    plt.scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 数据集exp2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6425d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上图可以用两个线性模型建模。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对前面的代码略作修改即可。算法的关键在于误差的计算。对于给定的数据集，应该先用线性的模型来对它进行拟合，然后计算真实的目标值与模型预测值间的差值。然后将这些差值的平方求和就得到了所需的误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearSolve(dataSet):\n",
    "    '''Process data set to target variable Y and independent variable X.'''\n",
    "    m, n = dataSet.shape\n",
    "    X = np.mat(np.ones((m, n)))\n",
    "    Y = np.mat(np.ones((m, 1)))\n",
    "    X[:, 1:n] = dataSet[:, 0:n-1]  # independent variable\n",
    "    Y = dataSet[:, -1]  # target variable\n",
    "    xTx = X.T * X\n",
    "    if np.linalg.det(xTx) == 0.0:\n",
    "        raise NameError(\"This matrix is singular, cannot do inverse, \\n\\\n",
    "                        try in creasing the second value of ops\")\n",
    "    ws = xTx.I * (X.T * Y)  # linear regression weights\n",
    "    return ws, X, Y\n",
    "\n",
    "\n",
    "def modelLeaf(dataSet):\n",
    "    '''Generate model of leaf when don not need to split data set.'''\n",
    "    ws, X, Y = linearSolve(dataSet)\n",
    "    return ws\n",
    "\n",
    "\n",
    "def modelErr(dataSet):\n",
    "    '''Calculate the square error of given data set.'''\n",
    "    ws, X, Y = linearSolve(dataSet)\n",
    "    yHat = X * ws\n",
    "    return np.sum(np.power(yHat - Y, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 第一个函数将数据集格式化成目标变量Y和自变量X。X和Y用于执行简单的线性回归。\n",
    "\n",
    "> 第二个函数与函数regLeaf()类似，当数据不需要切分的时候它负责生成叶节点的模型。该函数在数据集上调用linearSolve()并返回回归系数ws。\n",
    "\n",
    "> 第三个函数在给定数据集上计算误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left': matrix([[  1.69855694e-03],\n",
       "         [  1.19647739e+01]]), 'right': matrix([[ 3.46877936],\n",
       "         [ 1.18521743]]), 'spInd': 0, 'spVal': 0.28547699999999998}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata4 = loadDataSet('exp2.txt')\n",
    "createTree(mydata4, modelLeaf, modelErr, (1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成的两个线性模型分别是$y=3.468+1.1852x$和$y=0.0016985+11.96477x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x82e4518>,\n",
       " <matplotlib.lines.Line2D at 0x838a9b0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x82e4240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = []\n",
    "fr = open('exp2.txt')\n",
    "for line in fr.readlines():\n",
    "    curLine = line.rstrip().split('\\t')\n",
    "    fltLine = map(float, curLine)\n",
    "    data.append(fltLine)\n",
    "data = np.array(data)\n",
    "plt.scatter(data[:, 0], data[:, 1])\n",
    "x = np.linspace(0, 0.3)\n",
    "y = np.linspace(0.3, 1.0)\n",
    "plt.plot(x, 3.468+1.1852*x, 'r', y, 0.0016985+11.96477*y, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 示例：树回归与标准回归的比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use tree regression to forecast\n",
    "def regTreeEval(model, inData):  # regression tree\n",
    "    return float(model)\n",
    "\n",
    "\n",
    "def modelTreeEval(model, inData):  # model tree\n",
    "    n = inData.shape[1]\n",
    "    X = np.mat(np.ones((1, n+1)))\n",
    "    X[:, 1:n+1] = inData\n",
    "    return float(X*model)\n",
    "\n",
    "\n",
    "def treeForecast(tree, inData, modelEval=regTreeEval):\n",
    "    '''Forecast.\n",
    "    Args:\n",
    "        tree: tree\n",
    "            got by createTree()\n",
    "        inData: single data(a vector)\n",
    "            value to forecast\n",
    "        modelEval: regTreeEval, modelTreeEval, optional\n",
    "            the kind of tree\n",
    "    Returns:\n",
    "        a scalar, forecasting tree\n",
    "    '''\n",
    "    if not isTree(tree):  # only root node\n",
    "        return modelEval(tree, inData)\n",
    "    if inData[tree['spInd']] > tree['spVal']:\n",
    "        if isTree(tree['left']):\n",
    "            return treeForecast(tree['left'], inData, modelEval)\n",
    "        else:\n",
    "            return modelEval(tree['left'], inData)\n",
    "    else:\n",
    "        if isTree(tree['right']):\n",
    "            return treeForecast(tree['right'], inData, modelEval)\n",
    "        else:\n",
    "            return modelEval(tree['right'], inData)\n",
    "\n",
    "\n",
    "def createForecast(tree, testData, modelEval=regTreeEval):\n",
    "    '''Returns vector(forecasting value).'''\n",
    "    m = len(testData)\n",
    "    yHat = np.mat(np.zeros((m, 1)))\n",
    "    for i in range(m):\n",
    "        yHat[i, 0] = treeForecast(tree, np.mat(testData[i]), modelEval)\n",
    "    return yHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 对于单个的数据点或行向量，函数treeForecast()会返回一个浮点值。在给定树的情况下，对于单个数据点，该函数会给出一个预测值。\n",
    "\n",
    ">调用函数treeForecast()时需要指定树的类型，以便在叶节点上能够调用合适的模型。\n",
    "\n",
    ">回归树节点进行预测，就调用函数regTreeEval()；要对模型树节点进行预测，就调用modelTreeEval()。它们会对输入数据进行格式化处理，在原始数据矩阵上增加第0列，然后计算并返回预测值。\n",
    "\n",
    ">最后一个函数，会多次调用treeForecast()。能够以向量形式返回一组预测值，在对整个测试集进行预测时非常有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    trainMat = loadDataSet('bikeSpeedVsIq_train.txt')  # training set\n",
    "    testMat = loadDataSet('bikeSpeedVsIq_test.txt')    # testing set\n",
    "    # create a regression tree\n",
    "    myRegTree = createTree(trainMat, ops=(1, 20))\n",
    "    yRegHat = createForecast(myRegTree, testMat[:, 0])\n",
    "    print(np.corrcoef(yRegHat, testMat[:, 1], rowvar=0)[0, 1])\n",
    "    # create a model tree\n",
    "    myModTree = createTree(trainMat, modelLeaf, modelErr, ops=(1, 20))\n",
    "    yModHat = createForecast(myModTree, testMat[:, 0], modelTreeEval)\n",
    "    print(np.corrcoef(yModHat, testMat[:, 1], rowvar=0)[0, 1])\n",
    "    # standard line regression\n",
    "    ws, X, Y = linearSolve(trainMat)\n",
    "    yHat = np.mat(np.zeros((len(testMat), 1)))\n",
    "    for i in range(testMat.shape[0]):\n",
    "        yHat[i] = testMat[i, 0]*ws[1, 0] + ws[0, 0]\n",
    "    print(np.corrcoef(yHat, testMat[:, 1], rowvar=0)[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964085231822\n",
      "0.976041219138\n",
      "0.943468423567\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较哪种模型更好，一个客观的方法是计算相关系数($R^2$)，越接近1.0越好。\n",
    "可以看出树回归方法在预测复杂数据时会比简单的线性模型更有效。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
